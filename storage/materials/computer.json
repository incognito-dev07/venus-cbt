{
  "CSC101": {
    "name": "Computing Science",
    "icon": "fa-laptop-code",
    "topics": {
      "universal_machine": {
        "name": "Universal Machine",
        "content": "<h1>Universal Machine</h1>\n\n<h2>The Very Idea of the Universal Machine</h2>\n<p>A universal machine is a general-purpose symbol-manipulating machine, capable of any task that can be represented as a program. A program is an organized sequence of logical steps that directs the machine's operations. Any actual computer—from a smartphone to a supercomputer—approximates an ideal universal machine.</p>\n\n<p>This universality means that given enough time and memory, a universal machine can simulate any other computational device. This is why your laptop can run a calculator, a word processor, a web browser, and games—it's not specialized for any particular task but can be programmed to perform virtually any task by changing the software, not the hardware.</p>\n\n<h3>Universal versus Special-Purpose Machines</h3>\n<p><strong>Special-purpose machines</strong> are designed to perform specific tasks efficiently but cannot be easily adapted for other purposes. Their functionality is built into their hardware and cannot be changed without physical modification. Examples include:</p>\n<ul>\n<li><strong>Car</strong> - designed specifically for transportation</li>\n<li><strong>Microwave oven</strong> - designed exclusively for heating food</li>\n<li><strong>TV set</strong> - designed for displaying video content</li>\n<li><strong>Pocket calculator</strong> - designed for arithmetic calculations; its functions are hardwired</li>\n</ul>\n\n<p><strong>Computer (Universal Machine)</strong> - designed to perform any task that can be represented as a program. The same machine can be a word processor, a music player, a communication device, or a gaming platform—all by changing the software.</p>\n\n<p>The key insight is that universality comes from separating the machine's physical hardware from its logical instructions. The hardware provides the capability to execute instructions; the program determines which specific instructions are executed.</p>\n\n<h3>What is a Computer?</h3>\n<p>The term \"computer\" has evolved dramatically over time:</p>\n<ul>\n<li><strong>1926:</strong> \"One who computes\"—a person employed to perform calculations</li>\n<li><strong>1993:</strong> \"An apparatus built to perform routine calculations\"</li>\n<li><strong>Modern:</strong> A usually electronic device for storing and processing data according to instructions given in a variable program</li>\n</ul>\n\n<p>Computers are fundamentally machines for processing information: storage, retrieval, transformation, and analysis. Computer Science is the science of processing information by computers.</p>\n\n<h3>Early Devices to Facilitate Numerical Operations</h3>\n\n<p><strong>Abacus (about 200 B.C.):</strong> A mechanical aid used for counting and arithmetic. Addition, subtraction, division, and multiplication can be performed on a standard abacus. The frame has vertical rods with sliding beads—beads in the upper deck represent 5 units, lower deck beads represent 1 unit.</p>\n\n<p><strong>Planimeters (1906):</strong> Precision instruments for measuring the area of irregular shapes. By tracing the outline, a measuring wheel rotates and mechanically computes the area—an early analog computer.</p>\n\n<h3>Babbage's Engines: The First Vision of Programmable Computing</h3>\n\n<p><strong>Charles Babbage's Difference Engine (1833):</strong> Designed to mechanize the computation of mathematical tables using the method of differences (adding increments to intermediate results). This was a special-purpose calculator.</p>\n\n<p><strong>Charles Babbage's Analytical Engine (1871):</strong> The first design for a general-purpose computer, introducing revolutionary concepts:</p>\n<ul>\n<li><strong>Separation of store and mill:</strong> The \"store\" held numbers (memory), and the \"mill\" performed calculations (CPU)</li>\n<li><strong>Punched card programming:</strong> Cards for operations (instructions) and variables (data)</li>\n<li><strong>Conditional branching:</strong> The machine could make decisions</li>\n<li><strong>Loops and subroutines:</strong> Card sequences could be reused</li>\n<li><strong>Machine-readable libraries:</strong> Programs and data could be stored and reused</li>\n</ul>\n\n<p>The Analytical Engine would be able to carry out any mathematical operation by use of stored programs. Ada Lovelace wrote programs for it, making her the first programmer, and envisioned that such machines could eventually compose music and process symbols—not just numbers.</p>\n\n<h3>Babbage's Idea and Modern Computers</h3>\n<p>Babbage's separation of \"store\" and \"mill\" corresponds directly to modern computers:</p>\n<ul>\n<li><strong>Main memory (store)</strong> — where programs and data reside during execution</li>\n<li><strong>Central processing unit (CPU) (mill)</strong> — where arithmetic and logical operations are performed</li>\n</ul>\n\n<p>A modern stored-program machine can execute any sequence of instructions stored in its memory. Both instructions and data are represented as binary numbers and stored in the same memory. By loading different programs, the same hardware becomes whatever virtual machine the programmer wishes.</p>\n\n<h3>How Does a Stored-Program Machine Work?</h3>\n<p>The instruction pointer (program counter) is a special register that holds the address of the next instruction in memory. The control unit orchestrates the fetch-execute cycle:</p>\n\n<ol>\n<li><strong>Fetch:</strong> The control unit reads the instruction from the memory address in the instruction pointer</li>\n<li><strong>Decode:</strong> The instruction decoder interprets what operation needs to be performed</li>\n<li><strong>Execute:</strong> The appropriate hardware components (ALU, memory interface) perform the operation</li>\n<li><strong>Update instruction pointer:</strong> Unless the instruction was a branch, the pointer advances to the next instruction</li>\n</ol>\n\n<p>A branch instruction can change the instruction pointer, enabling decisions, loops, and subroutines—the building blocks of all programs.</p>\n\n<h2>Turing's Symbol Manipulator</h2>\n\n<p><strong>Alan Turing's Universal Machine (1936):</strong> Turing conceived an abstract mathematical model now called the Turing machine to answer fundamental questions about computation. It consists of:</p>\n<ul>\n<li>An infinite tape divided into cells, each containing a symbol</li>\n<li>A read/write head that can move left or right</li>\n<li>A finite set of states</li>\n<li>A set of rules (transition function) that determine, based on current state and symbol, what to write, which way to move, and what state to enter</li>\n</ul>\n\n<p>Despite its simplicity, a Turing machine can compute anything that is computable—any algorithm that can be executed by any computer can be executed by some Turing machine.</p>\n\n<p>The <strong>Universal Turing Machine (UTM)</strong> can imitate any other Turing machine. The description of any specific Turing machine M is encoded onto UTM's tape, then UTM reads M's rules as a program and simulates M's behavior. This is exactly how modern computers work: they read a program from memory and execute it.</p>\n\n<p><strong>Church-Turing Thesis:</strong> Anything computable (in the intuitive sense) can be computed by some Turing machine (or equivalently, by any programming language). This defines the theoretical limits of computation and shows that all computers are fundamentally equivalent in what they can compute.</p>\n\n<p><strong>Halting Problem:</strong> Turing proved that no algorithm can determine whether any given program will eventually halt or run forever. This demonstrates that there are fundamental limits to computation—some problems are uncomputable, no matter how powerful the machine.</p>"
      },
      "computer_anatomy": {
        "name": "Computer Anatomy",
        "content": "<h1>Computer Anatomy</h1>\n\n<h2>Layers of Virtual Machines</h2>\n<p>A computer system can be viewed as a hierarchy of layers, each providing abstraction that hides the complexity of the layers below:</p>\n<ul>\n<li><strong>Level 0: Digital Logic</strong> - Gates, circuits, flip-flops (physical electronics)</li>\n<li><strong>Level 1: Microarchitecture</strong> - Registers, ALU, data paths (how instructions are implemented)</li>\n<li><strong>Level 2: Instruction Set Architecture</strong> - Machine instructions the CPU understands</li>\n<li><strong>Level 3: Operating System</strong> - System calls, file management, process scheduling</li>\n<li><strong>Level 4: Assembly Language</strong> - Mnemonic representation of machine instructions</li>\n<li><strong>Level 5: High-Level Languages</strong> - C, Java, Python, etc. (human-readable)</li>\n<li><strong>Level 6: Application</strong> - User programs (word processors, browsers, games)</li>\n</ul>\n\n<p>Each layer treats the layer below as a virtual machine, providing services and hiding implementation details. This abstraction is what makes modern computing possible—programmers don't need to know about transistors to write applications.</p>\n\n<h2>The Von Neumann Architecture</h2>\n\n<p>Developed by John von Neumann and others in 1946/47, this architecture became the blueprint for virtually all computers. Key characteristics:</p>\n<ul>\n<li><strong>Program and data both reside in memory</strong> (stored-program concept)</li>\n<li><strong>Each memory location has an address</strong> for random access</li>\n<li><strong>Program commands are stored in consecutive memory locations</strong> (sequential execution by default)</li>\n<li><strong>Jump commands and conditional jumps</strong> enable loops, decisions, and subroutines</li>\n<li><strong>Binary number system</strong> is used for all representation</li>\n</ul>\n\n<h3>Main Components</h3>\n\n<ul>\n<li><strong>Arithmetic Logical Unit (ALU):</strong> Performs arithmetic (add, subtract, multiply, divide) and logical (AND, OR, NOT, XOR) operations</li>\n<li><strong>Memory:</strong> Stores both instructions and data (RAM)</li>\n<li><strong>Control Unit:</strong> Coordinates all activities, fetches and decodes instructions, generates control signals</li>\n<li><strong>Input/Output Unit:</strong> Handles communication with peripheral devices</li>\n</ul>\n\n<h3>CPU and RAM</h3>\n\n<p><strong>Central Processing Unit (CPU):</strong> The brain of the computer. Components include:</p>\n<ul>\n<li><strong>Control Unit:</strong> Directs operations, fetches and decodes instructions</li>\n<li><strong>Arithmetic Logic Unit (ALU):</strong> Performs calculations</li>\n<li><strong>Registers:</strong> Small, extremely fast storage locations within the CPU</li>\n</ul>\n\n<p><strong>Random Access Memory (RAM):</strong> Main memory that can be read from and written to. It is volatile—loses all data when power is turned off. Organized as words (typically 32-bit or 64-bit), each with a unique address.</p>\n\n<h4>How the ALU Works:</h4>\n<ol>\n<li>Data arrives from RAM and is held in registers</li>\n<li>Control Unit sends a signal indicating which operation to perform</li>\n<li>ALU performs the operation using its internal circuits</li>\n<li>Result is placed in the accumulator (or another register)</li>\n<li>Status flags are updated (Zero, Carry, Overflow, Negative)</li>\n</ol>\n\n<h3>Execution of Machine Instructions</h3>\n\n<ol>\n<li><strong>Fetch:</strong> Control unit sends the address from Program Counter to Memory Address Register; instruction is read into Memory Data Register, then copied to Instruction Register; Program Counter is incremented</li>\n<li><strong>Decode:</strong> Instruction decoder interprets the opcode and identifies operands</li>\n<li><strong>Execute:</strong> ALU performs arithmetic/logical operations, or memory access occurs, or branch target is calculated</li>\n<li><strong>Store:</strong> Results are written back to registers or memory; status flags are updated</li>\n</ol>\n\n<p>This cycle repeats millions or billions of times per second.</p>\n\n<h3>Machine Language</h3>\n\n<p><strong>Elementary operations:</strong> Data transfer (LOAD, STORE), arithmetic (ADD, SUB), logical (AND, OR), program control (JUMP, conditional branches), comparison, I/O.</p>\n\n<p><strong>Instruction Formats:</strong></p>\n<ul>\n<li><strong>1-address:</strong> OPCODE ADDRESS (e.g., ADD X - adds contents of memory location X to accumulator)</li>\n<li><strong>2-address:</strong> OPCODE ADDR1 ADDR2 (e.g., ADD A,B - adds A and B, stores result in A)</li>\n<li><strong>3-address:</strong> OPCODE ADDR1 ADDR2 ADDR3 (e.g., ADD A,B,C - adds A and B, stores in C)</li>\n</ul>\n\n<h4>Machine Language Example:</h4>\n<table border='1' cellpadding='5'>\n<tr><th>Opcode</th><th>Mnemo</th><th>Description</th></tr>\n<tr><td>0000</td><td>HLT</td><td>Halt program execution</td></tr>\n<tr><td>0001</td><td>LOA</td><td>Load operand to accumulator</td></tr>\n<tr><td>0010</td><td>STI</td><td>Store accumulator at address</td></tr>\n<tr><td>0011</td><td>ADD</td><td>Add operand to accumulator</td></tr>\n<tr><td>0100</td><td>SUB</td><td>Subtract operand from accumulator</td></tr>\n<tr><td>0101</td><td>MUL</td><td>Multiply operand with accumulator</td></tr>\n<tr><td>0111</td><td>JMP</td><td>Unconditional jump</td></tr>\n<tr><td>1000</td><td>JEZ</td><td>Jump if accumulator = 0</td></tr>\n</table>\n\n<p><strong>Example:</strong> The binary word 0011 0100 1001 1101 could be interpreted as:</p>\n<ul>\n<li>A decimal number: 13469</li>\n<li>A command: ADD with operand 157 → \"Add 157 to accumulator\"</li>\n</ul>\n\n<p>This ambiguity is fundamental—the CPU interprets memory content as instructions when fetched, but the same bits can be data when accessed by a data transfer instruction.</p>\n\n<h2>Hardware Components</h2>\n\n<h3>Central Components:</h3>\n<ul>\n<li><strong>CPU:</strong> Central Processing Unit (may have multiple cores)</li>\n<li><strong>RAM:</strong> Random Access Memory (main memory)</li>\n<li><strong>Motherboard:</strong> Provides physical support and electrical connections for all components; contains buses, chipset, expansion slots</li>\n</ul>\n\n<h3>Peripheral Devices:</h3>\n\n<p><strong>Input Devices:</strong> Keyboard, mouse, scanner, microphone, trackpad, joystick, touch screen</p>\n<p><strong>Output Devices:</strong> Monitor (LCD, LED, OLED), printer (inkjet, laser), speakers, plotter</p>\n<p><strong>Input-Output Devices:</strong> Hard disk (HDD), solid state drive (SSD), optical drives (CD/DVD/Blu-ray), modem, network card, USB flash drive</p>\n\n<h3>Important Registers in CPU:</h3>\n<ul>\n<li><strong>Accumulator:</strong> Stores ALU operation results</li>\n<li><strong>Program Counter (PC):</strong> Holds address of next instruction to execute</li>\n<li><strong>Instruction Register (IR):</strong> Holds current instruction being executed</li>\n<li><strong>Memory Address Register (MAR):</strong> Holds address for memory access</li>\n<li><strong>Memory Data Register (MDR):</strong> Holds data being transferred to/from memory</li>\n<li><strong>Stack Pointer:</strong> Points to top of stack in memory</li>\n<li><strong>Status Register (Flags):</strong> Indicates CPU status (Zero, Carry, Overflow, Negative)</li>\n<li><strong>General Purpose Registers:</strong> Temporary data storage</li>\n</ul>"
      },
      "operating_systems": {
        "name": "Operating Systems",
        "content": "<h1>Operating Systems</h1>\n\n<h2>Introduction</h2>\n<p><strong>Operating system (OS):</strong> A collection of programs that manages computer resources (processors, memory, I/O devices, storage). It acts like a conductor of an orchestra, ensuring all components work together harmoniously and conflicts are resolved.</p>\n\n<p>Without an OS, every program would need its own drivers for every device and would have to manage memory allocation—making software development virtually impossible. The OS provides essential services that abstract away hardware complexity.</p>\n\n<p>The OS provides a virtual machine that makes hardware easier to use:</p>\n<ul>\n<li><strong>Graphical user interface (GUI):</strong> Buttons, menus, icons for intuitive interaction</li>\n<li><strong>Virtual memory:</strong> Uses disk space to extend RAM, giving programs illusion of more memory</li>\n<li><strong>Multiprogramming:</strong> Creates illusion of running multiple programs simultaneously by rapid switching</li>\n<li><strong>File system:</strong> Organizes data on disks into files and folders</li>\n<li><strong>Device independence:</strong> Applications use devices without knowing hardware details</li>\n</ul>\n\n<h2>Evolution of Operating Systems</h2>\n\n<h3>First Generation (1945-1955): No OS</h3>\n<p>The only \"operating system\" was a human operator. Programmers signed up for machine time, manually set switches, and loaded programs. Extremely inefficient—valuable computer time was wasted while humans set up jobs.</p>\n\n<h3>Second Generation (1955-1965): Batch Systems</h3>\n<p>Operator collects a \"batch\" of programs (on punched cards or tape) and feeds them sequentially. A resident monitor automatically loads and runs each job. Reduced idle time between jobs, but CPU still idle during I/O.</p>\n\n<h3>Multiprogramming OS</h3>\n<p>Multiple programs kept in memory simultaneously. While one program waits for I/O (slow), another uses the CPU. This requires memory protection and interrupts.</p>\n<p>Enables:</p>\n<ul>\n<li><strong>Time sharing:</strong> CPU time divided into small slices, shared fairly among jobs</li>\n<li><strong>Interactive processing:</strong> Users get immediate response, illusion of dedicated machine</li>\n</ul>\n\n<h3>Multiuser OS</h3>\n<p>Many users share the same machine through time sharing (UNIX, Linux, Windows Server). Each user gets illusion of dedicated machine. Efficiency depends on CPU speed, time slice length, and user behavior.</p>\n\n<h3>Single-User Multiprogramming</h3>\n<p><strong>Cooperative multitasking:</strong> Programs must voluntarily yield CPU (early MacOS, Windows 3.x). One misbehaving program can freeze system.</p>\n<p><strong>Preemptive multitasking:</strong> OS switches between tasks at its own discretion using timer interrupts. All modern OS use this—you can print, browse web, and compile simultaneously without any task freezing the system.</p>\n\n<h3>Network OS</h3>\n<p>Enables sharing of resources over network: printers, files, specialized hardware, software licenses. Handles authentication, resource management, communication protocols.</p>\n\n<h2>Booting an Operating System</h2>\n\n<ol>\n<li>Power-on self-test (POST) checks hardware (CPU, RAM, basic devices)</li>\n<li>Boot strap program from ROM (BIOS/UEFI) enables access to boot devices</li>\n<li>Boot loader (e.g., GRUB) loads OS kernel into RAM</li>\n<li>Kernel initializes, runs startup scripts (CONFIG.SYS, AUTOEXEC.BAT, /etc/rc.d/*, Windows registry)</li>\n<li>Graphical user interface starts (if applicable)</li>\n<li>OS enters main loop, waiting for user input, network events, timer interrupts</li>\n</ol>\n\n<h2>O.S Architecture</h2>\n\n<h3>Command Processor (Shell)</h3>\n<p>Interface with users—either command-line (bash, cmd, PowerShell) or GUI (Windows Explorer, macOS Finder). Receives commands, interprets them, and requests program execution.</p>\n\n<h3>Scheduler</h3>\n<p>Manages execution of programs:</p>\n<ul>\n<li>Places programs in job queue (ready queue)</li>\n<li>Creates processes (active copies of programs in memory with own program counter and context)</li>\n<li>Decides which process runs next based on scheduling policy (round-robin, priority, etc.)</li>\n<li>Manages context switching between processes</li>\n</ul>\n\n<p><strong>Process:</strong> A program loaded into memory with its own program counter, stack, registers, and context. Multiple processes can run the same program. Process states: New, Ready, Running, Waiting, Terminated.</p>\n\n<h3>Resource Allocator</h3>\n<p>Ensures each process has needed resources: memory, files, devices, network connections. Prevents deadlocks (where processes wait indefinitely for resources held by each other).</p>\n\n<h3>Memory Manager</h3>\n<p>Coordinates memory usage:</p>\n<ul>\n<li>Tracks which memory areas are used by which processes</li>\n<li>Allocates and deallocates memory</li>\n<li>Provides virtual memory using swap files (pages swapped between RAM and disk)</li>\n<li>Protects processes from accessing each other's memory</li>\n<li>Handles paging and segmentation</li>\n</ul>\n\n<h3>I/O System Manager</h3>\n<p>Coordinates peripheral devices through device drivers. Provides:</p>\n<ul>\n<li>Device independence (uniform interface: open, read, write, close)</li>\n<li>Buffering, caching, spooling (queuing output for devices like printers)</li>\n<li>Error handling and retry</li>\n</ul>\n\n<h3>File Manager</h3>\n<p>Manages files on disks:</p>\n<ul>\n<li>Organizes files into directories (folders)</li>\n<li>Enforces access protection (permissions)</li>\n<li>Supports file sharing among processes/users</li>\n<li>Maps file names to physical disk locations</li>\n<li>Maintains file metadata (size, creation/modification time, owner)</li>\n<li>Manages disk space allocation</li>\n</ul>\n\n<p>Common file systems: NTFS (Windows), ext4 (Linux), APFS (macOS), FAT32.</p>\n\n<h3>Dispatcher</h3>\n<p>Performs context switching:</p>\n<ol>\n<li>Saves current process's context (registers, PC, stack pointer) to its process control block</li>\n<li>Loads saved context of next process</li>\n<li>Updates memory structures (page tables)</li>\n<li>Transfers control to new process</li>\n</ol>\n\n<h2>Types of Operating Systems</h2>\n<ul>\n<li><strong>Batch OS:</strong> Jobs processed sequentially with minimal interaction (payroll, billing)</li>\n<li><strong>Time-sharing OS:</strong> CPU time divided among users/tasks (UNIX, Linux, Windows)</li>\n<li><strong>Real-time OS (RTOS):</strong> Guaranteed response times for critical systems (VxWorks, QNX, FreeRTOS) - used in medical devices, avionics, industrial control</li>\n<li><strong>Distributed OS:</strong> Multiple computers appear as single system</li>\n<li><strong>Network OS:</strong> Provides services over network, but each computer runs own OS</li>\n<li><strong>Mobile OS:</strong> For smartphones/tablets (Android, iOS)</li>\n<li><strong>Embedded OS:</strong> Built into appliances, cars, IoT devices (embedded Linux, ThreadX)</li>\n</ul>\n\n<h2>Examples of Operating Systems</h2>\n<ul>\n<li><strong>DOS:</strong> Single-user, command-line (MS-DOS, PC-DOS)</li>\n<li><strong>Windows:</strong> Microsoft's GUI OS family (Windows 95/98/XP/Vista/7/8/10/11, Windows NT/Server)</li>\n<li><strong>UNIX:</strong> Multiuser, multitasking OS developed at Bell Labs (1969). Variants: System V, BSD, Solaris, AIX</li>\n<li><strong>Linux:</strong> Open-source Unix-like OS kernel (with GNU tools). Distributions: Ubuntu, Red Hat, Debian</li>\n<li><strong>macOS:</strong> Apple's Unix-based OS for Mac computers</li>\n<li><strong>Android/iOS:</strong> Mobile operating systems</li>\n</ul>"
      },
      "bits_arithmetic": {
        "name": "Bits and Arithmetic",
        "content": "<h1>Bits and Arithmetic</h1>\n\n<h2>Binary Data Representation</h2>\n\n<p><strong>Bit:</strong> The smallest unit of information (0 or 1). Everything in a computer—numbers, text, images, sound, video, programs—is ultimately represented as sequences of bits.</p>\n<p><strong>Byte:</strong> A group of 8 bits. A byte can represent 2⁸ = 256 different states. Memory is typically addressed in bytes.</p>\n<p><strong>Word:</strong> The number of bits a computer can process in one operation (32 or 64 bits in modern computers). Word size affects maximum memory addressable and operation speed.</p>\n\n<p>For n bits, we have 2ⁿ distinct bit patterns. What these patterns mean depends entirely on context. The same pattern 01000001 could represent:</p>\n<ul>\n<li>The number 65 (unsigned integer)</li>\n<li>The number -63 (two's complement)</li>\n<li>The ASCII character 'A'</li>\n<li>A pixel color component</li>\n<li>Part of a machine instruction</li>\n</ul>\n\n<h3>Units of Storage:</h3>\n<ul>\n<li><strong>Bit (b):</strong> Binary digit</li>\n<li><strong>Byte (B):</strong> 8 bits</li>\n<li><strong>Kilobyte (KB):</strong> 1024 bytes (2¹⁰)</li>\n<li><strong>Megabyte (MB):</strong> 1024 KB (2²⁰, ~1 million bytes)</li>\n<li><strong>Gigabyte (GB):</strong> 1024 MB (2³⁰, ~1 billion bytes)</li>\n<li><strong>Terabyte (TB):</strong> 1024 GB (2⁴⁰, ~1 trillion bytes)</li>\n</ul>\n\n<p>Note: Hard drive manufacturers often use decimal definitions (1 KB = 1000 bytes), causing confusion.</p>\n\n<h2>Number Systems for Computers</h2>\n\n<p>A number system has a base B and digits 0 to B-1. Value = ∑ d_i × Bⁱ</p>\n\n<table border='1' cellpadding='5'>\n<tr><th>System</th><th>Base</th><th>Digits</th><th>Use</th></tr>\n<tr><td>Binary</td><td>2</td><td>0,1</td><td>Fundamental hardware representation</td></tr>\n<tr><td>Octal</td><td>8</td><td>0-7</td><td>Compact binary (3 bits per digit); Unix file permissions</td></tr>\n<tr><td>Decimal</td><td>10</td><td>0-9</td><td>Human input/output</td></tr>\n<tr><td>Hexadecimal</td><td>16</td><td>0-9,A-F</td><td>Compact binary (4 bits per digit); memory dumps, colors, assembly</td></tr>\n</table>\n\n<h3>Binary Number System</h3>\n<p>Base 2, digits 0 and 1. Each position is a power of 2.</p>\n<p>Example: 10110₂ = 1×16 + 0×8 + 1×4 + 1×2 + 0×1 = 22₁₀</p>\n\n<h3>Hexadecimal Number System</h3>\n<p>Base 16, digits 0-9 and A-F (A=10, B=11, C=12, D=13, E=14, F=15).</p>\n<p>Example: 1F2₁₆ = 1×256 + 15×16 + 2×1 = 256 + 240 + 2 = 498₁₀</p>\n\n<h3>Conversions</h3>\n\n<p><strong>Decimal to Binary (repeated division by 2):</strong></p>\n<p>Example: Convert 25₁₀ to binary:</p>\n<pre>25 ÷ 2 = 12 remainder 1 (LSB)\n12 ÷ 2 = 6  remainder 0\n6 ÷ 2 = 3   remainder 0\n3 ÷ 2 = 1   remainder 1\n1 ÷ 2 = 0   remainder 1 (MSB)</pre>\n<p>Reading remainders upward: 11001₂</p>\n\n<p><strong>Binary to Decimal:</strong> Multiply each bit by its place value and sum.</p>\n<p>Example: 10110₂ = 1×16 + 0×8 + 1×4 + 1×2 + 0×1 = 22₁₀</p>\n\n<p><strong>Binary to Hex:</strong> Group bits in groups of 4 from right, convert each group.</p>\n<p>Example: 11010110₂ = 1101 0110 = D6₁₆</p>\n\n<p><strong>Hex to Binary:</strong> Replace each hex digit with 4 bits.</p>\n<p>Example: 1F2₁₆ = 0001 1111 0010₂</p>\n\n<h2>Binary Logic</h2>\n\n<h3>Basic Logic Gates:</h3>\n<table border='1' cellpadding='5'>\n<tr><th>Gate</th><th>Expression</th><th>Truth Table</th></tr>\n<tr><td>AND</td><td>Y = A·B</td><td>0·0=0, 0·1=0, 1·0=0, 1·1=1</td></tr>\n<tr><td>OR</td><td>Y = A+B</td><td>0+0=0, 0+1=1, 1+0=1, 1+1=1</td></tr>\n<tr><td>NOT</td><td>Y = ¬A</td><td>¬0=1, ¬1=0</td></tr>\n<tr><td>XOR</td><td>Y = A⊕B</td><td>0⊕0=0, 0⊕1=1, 1⊕0=1, 1⊕1=0</td></tr>\n<tr><td>NAND</td><td>Y = ¬(A·B)</td><td>0 NAND 0=1, 0 NAND 1=1, 1 NAND 0=1, 1 NAND 1=0</td></tr>\n<tr><td>NOR</td><td>Y = ¬(A+B)</td><td>0 NOR 0=1, 0 NOR 1=0, 1 NOR 0=0, 1 NOR 1=0</td></tr>\n</table>\n\n<h3>Binary Arithmetic</h3>\n\n<p><strong>Addition:</strong> 0+0=0, 0+1=1, 1+0=1, 1+1=0 carry 1; 1+1+1=1 carry 1</p>\n<p>Example: 1010₂ (10) + 1101₂ (13) = 10111₂ (23)</p>\n<pre>\n  1010\n+ 1101\n------\n 10111\n</pre>\n\n<p><strong>Subtraction:</strong> 0-0=0, 1-0=1, 1-1=0, 0-1=1 with borrow 1</p>\n<p>Example: 1101₂ (13) - 1010₂ (10) = 0011₂ (3)</p>\n\n<p><strong>Multiplication:</strong> Similar to decimal but simpler (multiply by 0 or 1).</p>\n<p>Example: 111₂ (7) × 101₂ (5) = 100011₂ (35)</p>\n\n<h2>Negative Numbers</h2>\n\n<h3>Signed Magnitude</h3>\n<p>Most significant bit is sign (0=positive, 1=negative). Problem: two representations for zero (+0 and -0).</p>\n\n<h3>1's Complement</h3>\n<p>Negative = invert all bits of positive. Still has two zeros. Addition requires end-around carry.</p>\n\n<h3>2's Complement (Most Common)</h3>\n<p>To get negative: invert all bits, add 1.</p>\n<p>Example with 8 bits: +9 = 00001001, -9 = 11110111</p>\n<p>Advantages:</p>\n<ul>\n<li>Only one representation for zero (00000000)</li>\n<li>Addition and subtraction use same hardware (just add)</li>\n<li>Range: -2ⁿ⁻¹ to 2ⁿ⁻¹-1 (8-bit: -128 to 127)</li>\n</ul>\n\n<p>With 2's complement, subtraction becomes addition: 14 - 7 = 14 + (-7)</p>\n\n<h2>Floating Point Numbers</h2>\n\n<p>Representation: value = mantissa × 2^exponent (scientific notation in binary)</p>\n\n<p><strong>IEEE 754 32-bit format (single precision):</strong></p>\n<ul>\n<li>Sign: 1 bit (0 positive, 1 negative)</li>\n<li>Exponent: 8 bits (biased by 127)</li>\n<li>Mantissa: 23 bits (with implicit leading 1, giving 24 bits precision)</li>\n</ul>\n\n<p>Value = (-1)^sign × 1.mantissa × 2^(exponent - 127)</p>\n\n<p><strong>Example:</strong> Represent 5.75₁₀ in 32-bit IEEE 754:</p>\n<ol>\n<li>Convert to binary: 5.75 = 101.11₂</li>\n<li>Normalize: 1.0111 × 2²</li>\n<li>Sign: positive → 0</li>\n<li>Exponent: 2 + 127 = 129 = 10000001₂</li>\n<li>Mantissa: 0111 followed by zeros to 23 bits</li>\n</ol>\n<p>Result: 0 10000001 01110000000000000000000</p>\n\n<p><strong>Limited precision:</strong> Not all numbers can be represented exactly (e.g., 0.1 has no exact binary representation). This causes rounding errors—never compare floating point numbers for exact equality; use tolerance.</p>\n\n<h2>Representation of Strings</h2>\n\n<h3>Character Encoding</h3>\n<ul>\n<li><strong>ASCII:</strong> 7-bit (128 characters) or 8-bit extended (256). Control characters (0-31), printable (32-126).</li>\n<li><strong>EBCDIC:</strong> IBM mainframe encoding, incompatible with ASCII.</li>\n<li><strong>Unicode:</strong> Universal standard representing all world writing systems. Encodings: UTF-8 (variable length, web standard), UTF-16 (Java, Windows), UTF-32 (fixed 4 bytes).</li>\n</ul>\n\n<h3>String Storage Methods</h3>\n<ol>\n<li><strong>Fixed length:</strong> Predetermined length for all strings (simple but wasteful).</li>\n<li><strong>Null-terminated:</strong> End marked with 0 byte (C strings). Simple but can't contain null within string, requires scanning for length.</li>\n<li><strong>Length-prefixed:</strong> First byte(s) store length, followed by characters (Pascal strings). Length known immediately, can contain any characters.</li>\n</ol>"
      },
      "problem_solving": {
        "name": "Problem Solving",
        "content": "<h1>Problem Solving</h1>\n\n<h2>Problem-Solving Strategies</h2>\n\n<p>Finding algorithmic solutions to programming problems requires the same skills of observation, deduction, and creative thinking used in solving mysteries.</p>\n\n<h3>Analytical (\"Hercule Poirot\")</h3>\n<ul>\n<li><strong>Careful laying out of evidence:</strong> Gather all available information. What are the inputs? What outputs are expected? What are the constraints?</li>\n<li><strong>Logically connecting clues:</strong> Identify patterns and relationships between different parts of the problem.</li>\n<li><strong>Systematic step-by-step reasoning:</strong> Break the problem into smaller pieces and solve each methodically.</li>\n</ul>\n\n<h3>Analogical (\"Jane Marple\")</h3>\n<ul>\n<li><strong>Finding solutions by analogy:</strong> Recognize that the current problem resembles one you've solved before.</li>\n<li><strong>Recognizing patterns:</strong> Build a mental library of problem patterns and solution templates.</li>\n<li><strong>Adapting known solutions:</strong> Modify previous solutions to fit the new context.</li>\n</ul>\n\n<p>This is why experienced programmers are more productive—they've seen more problems and have more analogies to draw upon.</p>\n\n<h3>Trial and Error (Hacking)</h3>\n<p>Making random attempts without understanding the problem. Not recommended for beginners because it doesn't build understanding and can introduce subtle bugs.</p>\n\n<h2>Algorithms</h2>\n\n<p><strong>Algorithm:</strong> A precise specification of steps to solve a problem. The word comes from the Persian mathematician Al-Khwarizmi.</p>\n\n<p><strong>Properties of Algorithms (Knuth):</strong></p>\n<ul>\n<li><strong>Finiteness:</strong> Must terminate after a finite number of steps (no infinite loops)</li>\n<li><strong>Definiteness:</strong> Each step must be precisely defined and unambiguous</li>\n<li><strong>Input:</strong> Zero or more quantities are supplied externally</li>\n<li><strong>Output:</strong> At least one quantity is produced</li>\n<li><strong>Effectiveness:</strong> Each operation must be basic enough to be done exactly in finite time</li>\n</ul>\n\n<p><strong>Coding:</strong> Translation of algorithm into a programming language.</p>\n<p><strong>Program:</strong> A step-by-step execution plan a computer can perform.</p>\n\n<h2>Givens, Goals, and Resources</h2>\n\n<p>Every problem can be analyzed in terms of three components:</p>\n<ul>\n<li><strong>Givens:</strong> Initial conditions (the way things are). What information do we start with?</li>\n<li><strong>Goals:</strong> Desired state (the way things should be). What constitutes a solution?</li>\n<li><strong>Resources:</strong> Means to transform givens into goals. What operations are allowed?</li>\n</ul>\n\n<p><strong>Examples:</strong></p>\n<ul>\n<li><strong>Game playing:</strong> Given = initial setup, Goal = win, Resources = allowed moves</li>\n<li><strong>Preparing a meal:</strong> Given = ingredients, Goal = cooked dish, Resources = recipe, cooking techniques</li>\n<li><strong>Sorting:</strong> Given = unsorted array, Goal = sorted array, Resources = comparisons, swaps</li>\n</ul>\n\n<h2>Analyzing Problem Spaces</h2>\n\n<p><strong>Problem space:</strong> Complete set of possible states reachable from givens through resources.</p>\n\n<h3>Questions for Preliminary Analysis:</h3>\n\n<p><strong>What are the givens?</strong></p>\n<ul>\n<li>Do we have all of them? Can we generalize?</li>\n<li>Is there a good notation to represent states?</li>\n</ul>\n\n<p><strong>What is the goal?</strong></p>\n<ul>\n<li>Single goal or multiple? Can it be split into subgoals?</li>\n<li>Are there obstacles or constraints?</li>\n</ul>\n\n<p><strong>What are the resources?</strong></p>\n<ul>\n<li>What are preconditions? What changes when applied (variants)?</li>\n<li>What stays the same (invariants)?</li>\n</ul>\n\n<h2>From Blind Search to Informed Search</h2>\n\n<p><strong>Blind Search (Uninformed):</strong> No guidance about which moves are better.</p>\n<ul>\n<li><strong>Depth-first search:</strong> Explore one path completely before backtracking (uses stack). Memory efficient but may get stuck in deep paths.</li>\n<li><strong>Breadth-first search:</strong> Explore all paths level by level (uses queue). Guarantees shortest path but may use much memory.</li>\n<li><strong>Generate and test:</strong> Generate possible solutions and test if they satisfy goal.</li>\n</ul>\n\n<p><strong>Informed Search (Heuristic):</strong> Uses domain knowledge to guide search.</p>\n<ul>\n<li><strong>Hill climbing:</strong> Always move to neighbor that appears best (like climbing in fog). May get stuck on local maxima.</li>\n<li><strong>Best-first search:</strong> Priority queue of states ordered by heuristic value.</li>\n<li><strong>A* algorithm:</strong> Combines actual cost with estimated cost to goal; optimal with good heuristic.</li>\n</ul>\n\n<p><strong>Example: Fox-Corn-Goose Problem</strong></p>\n<p>Farmer needs to transport fox, goose, and corn across river. Boat carries farmer + one item. Constraints: fox can't be alone with goose, goose can't be alone with corn. State = (farmer side, fox side, goose side, corn side). Search space has 16 states; solution found by exploring.</p>\n\n<h2>Analytical Reasoning: Top-Down Design</h2>\n\n<p>Breaking complex problems into smaller, manageable parts—similar to top-down design in programming: start with big picture, successively refine details.</p>\n\n<h3>Example: Planning a Menu</h3>\n<pre>\nI. Preceding Courses\n   A. Soup course\n      1. Cream of lettuce\n   B. Fish course\n      1. Filet of sole\nII. Main Course\n   A. Main dish\n      1. Cornish game hen\n   B. Accompaniments\n      1. Creamed spinach\n      2. Wild rice\nIII. Following Courses\n   A. Salad course\n   B. Dessert\n</pre>\n\n<p>Similarly, programs are developed top-down: start with main tasks, break into subtasks until operations are simple enough to code. Bottom-up development (building components first) is also possible; experienced developers use both.</p>\n\n<h2>The Analogical Approach</h2>\n\n<p>Using similarities between problems to find solutions. Common in machine learning (case-based reasoning).</p>\n\n<p><strong>Steps:</strong></p>\n<ol>\n<li>Recognize current problem resembles previously solved one</li>\n<li>Recall solution to previous problem</li>\n<li>Adapt that solution to fit current problem</li>\n<li>Verify adapted solution works</li>\n</ol>\n\n<h2>Developing Problem-Solving Skills</h2>\n\n<ol>\n<li><strong>Practice regularly:</strong> Solve programming puzzles, implement algorithms from scratch</li>\n<li><strong>Study solutions:</strong> After solving, look at others' solutions to learn different approaches</li>\n<li><strong>Reflect on process:</strong> What strategies worked? What didn't? How could you improve?</li>\n<li><strong>Build a toolkit:</strong> Collect problem patterns and solution templates</li>\n<li><strong>Collaborate:</strong> Explain your thinking to others; listen to how they approach problems</li>\n<li><strong>Break problems down:</strong> Practice decomposition regularly</li>\n<li><strong>Start simple:</strong> Solve a simplified version first, then add complexity</li>\n<li><strong>Draw diagrams:</strong> Visual representations often clarify relationships and flows</li>\n</ol>\n\n<p>Remember: Programming languages are tools; problem-solving is the craft. Mastering the craft is what makes a true programmer.</p>"
      },
      "programming_languages": {
        "name": "Programming Languages",
        "content": "<h1>Programming Languages</h1>\n\n<h2>Interpreters vs. Compilers</h2>\n\n<h3>Interpreters</h3>\n<ul>\n<li>Translate and execute source code line by line, without producing a separate machine-code file</li>\n<li>Good for rapid prototyping and interactive development—changes can be tested immediately</li>\n<li>Errors detected at runtime (during execution)</li>\n<li>Generally slower than compiled code because translation happens during execution</li>\n<li>Easier to implement and provide interactive environments</li>\n<li>Examples: Python, JavaScript, Ruby, PHP, early BASIC</li>\n</ul>\n\n<h3>Compilers</h3>\n<ul>\n<li>Translate entire program to machine code before execution, producing an executable file</li>\n<li>Faster execution because translation is done once and code runs directly on hardware</li>\n<li>Errors caught during compilation (syntax errors, type errors) before program ever runs</li>\n<li>Requires more type information from programmer (declarations)</li>\n<li>Examples: C, C++, Go, Rust, Pascal, FORTRAN; Java compiles to bytecode (then interpreted/JIT)</li>\n</ul>\n\n<p>Many modern languages use a hybrid approach: compilation to intermediate bytecode, then interpretation or just-in-time (JIT) compilation (Java, C#).</p>\n\n<h2>How Compilers Produce Machine Code</h2>\n\n<h3>Stages of Translation:</h3>\n\n<strong>1. Preprocessor:</strong>\n<ul>\n<li>Handles directives (#include, #define, #ifdef)</li>\n<li>Inserts header files, expands macros</li>\n<li>Removes comments</li>\n<li>Produces expanded source code</li>\n</ul>\n\n<strong>2. Compiler Front End:</strong>\n<ul>\n<li><strong>Lexical analysis (scanning):</strong> Converts source code to tokens (keywords, identifiers, operators, literals)</li>\n<li><strong>Syntax analysis (parsing):</strong> Checks token sequence against grammar rules, builds parse tree</li>\n<li><strong>Semantic analysis:</strong> Type checking, scope resolution, variable declarations</li>\n<li><strong>Intermediate code generation:</strong> Produces machine-independent representation (e.g., three-address code)</li>\n</ul>\n\n<strong>3. Optimizer:</strong>\n<ul>\n<li>Analyzes intermediate code to improve performance and reduce size</li>\n<li>Constant folding, dead code elimination, loop optimizations, inlining</li>\n</ul>\n\n<strong>4. Code Generator (Back End):</strong>\n<ul>\n<li>Translates optimized intermediate code to target machine code</li>\n<li>Register allocation, instruction selection, scheduling</li>\n<li>Produces object code (relocatable machine code)</li>\n</ul>\n\n<strong>5. Linker:</strong>\n<ul>\n<li>Combines object code with library code (e.g., I/O functions, math library)</li>\n<li>Resolves external references (calls to functions in other files)</li>\n<li>Produces executable file with relative addressing</li>\n</ul>\n\n<strong>6. Loader:</strong>\n<ul>\n<li>Loads executable into memory</li>\n<li>Performs relocation (adjusts addresses based on where program is loaded)</li>\n<li>Starts program execution</li>\n</ul>\n\n<h2>Describing Syntactic Structures: BNF</h2>\n\n<p><strong>Backus-Naur Form (BNF)</strong> was invented in the late 1950s to describe programming language syntax (ALGOL 60). It uses production rules:</p>\n<ul>\n<li><strong>Terminals:</strong> Actual symbols that appear in the language (keywords, operators, punctuation)</li>\n<li><strong>Non-terminals:</strong> Abstract syntactic categories (enclosed in &lt;&gt;)</li>\n<li><strong>Production rules:</strong> non-terminal ::= replacement (| means \"or\")</li>\n</ul>\n\n<h3>Example 1: English Sentences</h3>\n<pre>\nsentence ::= subject_part predicate_part\nsubject_part ::= extended_noun noun_with_attribute\npredicate_part ::= predicate object\npredicate ::= verb\nobject ::= article noun | article adjective noun\nextended_noun ::= \"The students\" | \"The TAs\"\nverb ::= \"like\" | \"enjoy\"\narticle ::= \"the\"\nadjective ::= \"weekly\" | \"daily\"\nnoun ::= \"lecture\" | \"labs\"\n</pre>\n<p>Generates: \"The students enjoy the weekly labs.\"</p>\n\n<h3>Example 2: Arithmetic Expressions</h3>\n<pre>\nE ::= T + E | T - E | T        (Expression)\nT ::= F * T | F / T | F        (Term)\nF ::= (E) | N                  (Factor)\nN ::= D N | D                  (Number)\nD ::= 0|1|2|3|4|5|6|7|8|9      (Digit)\n</pre>\n<p>This grammar encodes operator precedence (multiplication before addition). The parse tree determines evaluation order.</p>\n\n<h2>Programming Language Generations</h2>\n\n<p><strong>1GL - Machine Language:</strong> Binary code directly executed. Machine dependent, extremely tedious. Example: 10110000 01100001</p>\n\n<p><strong>2GL - Assembly Language:</strong> Mnemonic codes (MOV, ADD), symbolic addresses. Assembler translates to machine code. Still machine dependent. Example: MOV AL, 61h</p>\n\n<p><strong>3GL - High-Level Languages:</strong> English-like statements, machine independent. Use compilers/interpreters. Examples: C, Java, Python, FORTRAN, COBOL, Pascal</p>\n\n<p><strong>4GL - Very High-Level Languages:</strong> Domain-specific, non-procedural (what to do, not how). Examples: SQL, MATLAB, Visual Basic</p>\n\n<p><strong>5GL - Artificial Intelligence Languages:</strong> Logic and constraint-based. Examples: Prolog, LISP</p>\n\n<h2>Programming Paradigms</h2>\n\n<ul>\n<li><strong>Procedural/Imperative:</strong> Sequence of statements that change state (C, Pascal, FORTRAN)</li>\n<li><strong>Object-Oriented:</strong> Objects containing data and methods; encapsulation, inheritance, polymorphism (C++, Java, C#, Python)</li>\n<li><strong>Functional:</strong> Mathematical functions, no side effects, immutable data (Haskell, LISP, Scheme)</li>\n<li><strong>Logic:</strong> Facts and rules; computation as logical deduction (Prolog)</li>\n<li><strong>Event-Driven:</strong> Program flow determined by events (JavaScript, Visual Basic)</li>\n</ul>\n\n<p>Many modern languages are multi-paradigm (Python, C++, JavaScript, Scala).</p>\n\n<h2>Common Programming Languages</h2>\n\n<ul>\n<li><strong>FORTRAN (1957):</strong> Scientific and engineering computing</li>\n<li><strong>COBOL (1959):</strong> Business data processing</li>\n<li><strong>BASIC (1964):</strong> Beginner-friendly</li>\n<li><strong>C (1972):</strong> Systems programming, operating systems</li>\n<li><strong>C++ (1983):</strong> Object-oriented extension of C</li>\n<li><strong>Java (1995):</strong> Platform-independent, enterprise applications, Android</li>\n<li><strong>C# (2000):</strong> Microsoft's OO language for .NET</li>\n<li><strong>Python (1991):</strong> General-purpose, interpreted, data science, web, scripting</li>\n<li><strong>JavaScript (1995):</strong> Web development (client-side and server-side with Node.js)</li>\n<li><strong>PHP (1995):</strong> Server-side web development</li>\n<li><strong>SQL (1974):</strong> Database queries (4GL)</li>\n<li><strong>Swift (2014):</strong> Apple's modern language for iOS/macOS</li>\n<li><strong>Go (2009):</strong> Google's systems language for concurrent programming</li>\n<li><strong>Rust (2010):</strong> Safe systems programming focused on memory safety</li>\n</ul>\n\n<h2>Language Translators</h2>\n\n<ul>\n<li><strong>Assembler:</strong> Assembly language → machine code</li>\n<li><strong>Compiler:</strong> High-level language → machine code (all at once)</li>\n<li><strong>Interpreter:</strong> High-level language → execution line by line</li>\n<li><strong>Source Program:</strong> Original high-level code</li>\n<li><strong>Object Program:</strong> Translated machine code (or intermediate code)</li>\n</ul>\n\n<h2>Types of Programming Errors</h2>\n\n<ul>\n<li><strong>Syntax Errors:</strong> Violate language grammar rules (missing semicolon, unmatched parentheses). Caught by compiler/interpreter.</li>\n<li><strong>Semantic Errors:</strong> Meaning errors (type mismatch, undeclared variable, wrong number of arguments). Caught by compiler.</li>\n<li><strong>Logical Errors:</strong> Program runs but produces wrong results. Hardest to find; no error messages.</li>\n<li><strong>Runtime Errors:</strong> Occur during execution (division by zero, null pointer, file not found).</li>\n</ul>\n\n<p><strong>Debugging:</strong> Process of locating and fixing errors. Techniques: debuggers (breakpoints, variable inspection), code reviews, print statements, unit testing, rubber duck debugging.</p>"
      }
    }
  }
}